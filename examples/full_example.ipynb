{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering redshifts with *yet_another_wizz*\n",
    "\n",
    "This notebooks summarises the steps to compute clustering redshifts for an\n",
    "unknown sample of galaxies using a reference sample wit known redshifts.\n",
    "Additionally, a correction for the sample galaxy bias of the reference sample is\n",
    "applied (see Eqs. 17 & 20 in\n",
    "[van den Busch et al. 2020](https://arxiv.org/pdf/2007.01846)).\n",
    "\n",
    "This involves a number of steps:\n",
    "1. Preparing the input data (creating randoms, applying masks).\n",
    "2. Split it into spatial patches and cache them on disk for faster access.\n",
    "3. Computing the autocorrelation function amplitude $w_{\\rm ss}(z)$, used as\n",
    "   correction for the galaxy bias\n",
    "4. Computing the cross-correlation function amplitude $w_{\\rm sp}(z)$, which is\n",
    "   the biased redshift estimate.\n",
    "5. Summarising the result by correcting for the refernece sample bias and\n",
    "   produce a redshift PDF.\n",
    "6. Removing the cached data which are no longer needed.\n",
    "\n",
    "The aim of this notebook is to **give an overview of the wrapper functionality**,\n",
    "including a summary of all currently implemented optional parameters (commented).\n",
    "It is not meant to be a demonstaration of the performance of *yet_another_wizz*\n",
    "since the example data used here is very small and the resulting signal-to-noise\n",
    "ratio is quite poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that no cached data is present if a previous run has been interrupted.\n",
    "# TODO: properly implement option to overwrite an exsisting cache directory.\n",
    "def clean_up():\n",
    "    from shutil import rmtree\n",
    "    for key in (\"ref\", \"unk\"):\n",
    "        try:\n",
    "            rmtree(f\"test_{key}\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "clean_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the required stages from the *yet_another_wizz* wrapper. Each of\n",
    "the stages maps to one of the steps listed in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1, preparing the example data\n",
    "from yaw import UniformRandoms\n",
    "from rail.yaw_rail.utils import get_dc2_test_data\n",
    "\n",
    "from rail.yaw_rail import (\n",
    "    YawCacheCreate,     # step 2\n",
    "    YawAutoCorrelate,   # step 3\n",
    "    YawCrossCorrelate,  # step 4\n",
    "    YawSummarize,       # step 5\n",
    "    YawCacheDrop,       # step 6\n",
    ")  # equivalent: from rail.yaw_rail import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = \"debug\"  # verbosity level of built-in logger, disable with \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "Since this is a simple example, we are not going into the details of creating\n",
    "realistic randoms and properly masking the reference and unknown data to a\n",
    "shared footprint on sky. Instead, we are using a simulated dataset that serves\n",
    "as both, reference and unknown sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the small test dataset derived from 25 sqdeg of DC2,\n",
    "containing 100k objects on a limited redshift range of $0.2 < z < 1.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = get_dc2_test_data()  # downloads test data, cached for future calls\n",
    "redshifts = mock_data[\"z\"].to_numpy()\n",
    "zmin = redshifts.min()\n",
    "zmax = redshifts.max()\n",
    "n_data = len(mock_data)\n",
    "f\"N={n_data}, {zmin:.1f}<z<{zmax:.1f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we generate a x10 enhanced uniform random dataset for the test data\n",
    "constrained to its rectangular footprint. We add redshifts by cloning the\n",
    "redshift column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angular_rng = UniformRandoms(\n",
    "    mock_data[\"ra\"].min(),\n",
    "    mock_data[\"ra\"].max(),\n",
    "    mock_data[\"dec\"].min(),\n",
    "    mock_data[\"dec\"].max(),\n",
    "    seed=12345,\n",
    ")\n",
    "mock_rand = angular_rng.generate(n_data * 10, draw_from=dict(z=redshifts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting and caching the data\n",
    "\n",
    "This step is crucial to compute consistent clustering redshift uncertainties.\n",
    "*yet_another_wizz* uses spatial (jackknife) resampling and therefore, every\n",
    "input dataset must be split into the same exact spatial regions/patches. To\n",
    "improve the parallel performance, the datasets and randoms are pre-arranged into\n",
    "these patches and cached on disk for better random patch-wise access. While this\n",
    "is slower for small datasets, it is highly beneficial for large datasets with\n",
    "many patches and/or memory constraints.\n",
    "\n",
    "The RAIL wrapper uses manually specified cache directories, each of which contains\n",
    "one dataset and optionally corresponding randoms. This ensures that the patch\n",
    "centers are defined consistently. To create a new cache, use the\n",
    "`YawCacheCreate.create()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reference data\n",
    "\n",
    "To create a cache directory we must specify a path to the directory at which the\n",
    "data will be cached. This directory must not exist yet. We also have to specify\n",
    "a name for the stage to ensure that the reference and unknown caches (see below)\n",
    "are properly aliased and distinguishable in the RAIL data store.\n",
    "\n",
    "Furthermore, a few basic column names that describe the tabular input data must\n",
    "be provided. These are right ascension (`ra_name`) and declination (`dec_name`),\n",
    "and in case of the reference sample also the redshifts (`redshift_name`).\n",
    "\n",
    "Finally, the patches must be defined and there are three ways to do so:\n",
    "1. Generating a given number of patches from the object positions (peferrably of\n",
    "   the randoms if possible) using k-means clustering by specifying `n_patches`.\n",
    "2. Providing a column name in the input table which contains patch indices\n",
    "   (using 0-based indexing) by specifying `patch_name`.\n",
    "3. Using the patch centers from a different cache instance, given by its path in\n",
    "   the file system, by specifying `patches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_cache_ref = YawCacheCreate.make_stage(\n",
    "    name=\"ref\",\n",
    "    path=\"./test_ref\",\n",
    "    ra_name=\"ra\",\n",
    "    dec_name=\"dec\",\n",
    "    redshift_name=\"z\",\n",
    "    # weight_name=,\n",
    "    # patches=,\n",
    "    # patch_name=,\n",
    "    n_patches=5,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")\n",
    "cache_ref = stage_cache_ref.create(\n",
    "    data=mock_data,\n",
    "    rand=mock_rand,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The unknown data\n",
    "\n",
    "The same procedure for the unknown sample, however there are some small, but\n",
    "important differences. We use a different `path` and `name`, do not specify the\n",
    "`redshift_name` (since that is in reality not known), and here we chose to not\n",
    "provide any randoms for the unknown sample and instead rely on the reference\n",
    "sample randoms for cross-correlation measurements.\n",
    "\n",
    "Most importantly, we must provide the path the reference data cache,\n",
    "(`patches=\"./test_ref\"`) since we must ensure that the patch centers are\n",
    "identical. Even if the reference and unknown data are identical as in this case,\n",
    "the automatically generated patch centers (`n_patches`) are not deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_cache_unk = YawCacheCreate.make_stage(\n",
    "    name=\"unk\",\n",
    "    path=\"./test_unk\",\n",
    "    ra_name=\"ra\",\n",
    "    dec_name=\"dec\",\n",
    "    # redshift_name=,\n",
    "    # weight_name=,\n",
    "    patches=\"./test_ref\",\n",
    "    # patch_name=,\n",
    "    # n_patches=,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")\n",
    "cache_unk = stage_cache_unk.create(\n",
    "    data=mock_data,\n",
    "    # rand=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing the autocorrelation / bias correction\n",
    "\n",
    "The bias correction is computed from the amplitude of the angular autocorrelation\n",
    "function of the reference sample. The measurement parameters are the same as for\n",
    "the cross-correlation amplitude measurement, so we can define all parameters\n",
    "once in a dictionary.\n",
    "\n",
    "As a first step, we need to decide on which redshift bins we want to compute the\n",
    "clustering redshifts. Here we choose the redshift limits of the reference data\n",
    "(`zmin`/`zmax`) and, since the sample is small, only 8 bins (`zbin_num`) spaced\n",
    "linearly in redshift (`method=\"linear\"`). Finally, we have to define the physical\n",
    "scales in kpc (`rmin`/`rmax`, converted to angular separation at each redshift)\n",
    "on which we measure the correlation amplitudes.\n",
    "\n",
    "\n",
    "**Optional parameters:** Bins edges can alternatively specifed manually through\n",
    "`zbins`. To apply scale dependent weights, e.g. $\\propto r^{-1}$, specify\n",
    "`rweight=-1`. The parameter `rbin_num` specifies the resolution of the radial\n",
    "weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_config = dict(\n",
    "    rmin=100,   # in kpc\n",
    "    rmax=1000,  # in kpc\n",
    "    # rweight=None,\n",
    "    # rbin_num=50,\n",
    "    zmin=zmin,\n",
    "    zmax=zmax,\n",
    "    zbin_num=8,  # default: 30\n",
    "    # method=\"linear\",\n",
    "    # zbins=np.linspace(zmin, zmax, zbin_num+1)\n",
    "    # crosspatch=True,\n",
    "    # thread_num=None,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the autocorrelation using the `YawAutoCorrelate.correlate()` method,\n",
    "which takes a single parameter, the cache (handle) of the reference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ss = YawAutoCorrelate.make_stage(**corr_config).correlate(\n",
    "    sample=cache_ref,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the code is progressing, we can observe the log messages of *yet_another_wizz*\n",
    "which indicate the performed steps. Getting the cached data, generating a pairs\n",
    "of spatial patches to correlate, and counting pairs. Finally, the pair counts\n",
    "are stored as custom data handle in the data store.\n",
    "\n",
    "We can interact with these\n",
    "manually if we want to investigate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_ss = w_ss.data\n",
    "counts_ss.dd  # 5 patches by 5 patches, in 8 redshift bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computing the cross-correlation / redshift estimate\n",
    "\n",
    "The cross-correlation amplitude, which is the biased estimate of the unknown\n",
    "redshift distribution, is computed similarly to the autocorrelation above. We\n",
    "measure the autocorrelation using the `YawCrossCorrelate.correlate()` method,\n",
    "which takes two parameters, the cache (handle) of the reference data and the\n",
    "unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sp = YawCrossCorrelate.make_stage(**corr_config).correlate(\n",
    "    reference=cache_ref,\n",
    "    unknown=cache_unk,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can see the actions performed by *yet_another_wizz*. The main difference for the cross-correlation function is, that the second sample (the unknown data/randoms) are not binned by redshift when counting pairs.\n",
    "\n",
    "As before, we can interact with the result, e.g. by evaluating the correlation estimator and getting the cross-correlation amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_sp = w_sp.data\n",
    "corr_sp = counts_sp.sample()  # evaluate the correlation estimator\n",
    "corr_sp.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computing the redshift PDF\n",
    "\n",
    "The final analysis step is combining the two measured correlation amplitudes to\n",
    "get a redshift estimate which is corrected for the reference sample bias and\n",
    "converting it to a proper PDF. The latter is not trivial and different methods\n",
    "have been tested in the literature to remove negative correlation amplitudes.\n",
    "For simplicity, there is currently just one method implemented in the\n",
    "*yet_another_wizz* wrapper, which sets all negative or non-finite values to\n",
    "zero.\n",
    "\n",
    "We use `YawSummarize.summarize()` method, which takes the pair count handles of\n",
    "the cross- and autocorrelation functions as input. In principle, the\n",
    "autocorrelation of the unknown sample could be specified to fully correct for\n",
    "galaxy bias, however this is not possible in practice since the exact redshifts\n",
    "of the unknown objects are not known.\n",
    "\n",
    "All stage parameters are optional and usually not required. They can be used to\n",
    "customise the correlation estimators used for each correlation function. They\n",
    "default to Landy-Szalay (`\"LS\"`) where possible, otherwise Davis-Peebles (`\"DP\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate = YawSummarize.make_stage(\n",
    "    # cross_est=,\n",
    "    # ref_est=,\n",
    "    # unk_est=,\n",
    "    # crosspatch=True,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ").summarize(\n",
    "    cross_corr=w_sp,\n",
    "    ref_corr=w_ss,  # default: None\n",
    "    # unk_corr=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stage produces two outputs, a `qp.Ensemble`, tagged as `output` which\n",
    "contains jackknife samples of the clustering redshift estimate will all negative\n",
    "amplitudes set to zero, and the uncorrected redshift estimate, tagged as\n",
    "`yaw_cc`. Below is a comparison plot of the true redshift histogram, the\n",
    "raw estimate from *yet_another_wizz* and the first jackknife sample stored in\n",
    "the `qp.Ensemble`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble, yaw_cc = estimate[\"output\"].data, estimate[\"yaw_cc\"].data\n",
    "\n",
    "ax = ensemble.plot(xlim=[zmin, zmax], label=\"qp, sample 1\")\n",
    "mock_data.hist(\"z\", bins=w_sp.data.edges, density=True, color=\"0.8\", ax=ax, label=\"true\")\n",
    "yaw_cc.normalised().plot(label=\"yet_another_wizz\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Remove caches\n",
    "\n",
    "The cached datasets are not automatically removed, since the algorithm does not\n",
    "know when they are no longer needed. Additionally, the reference data could be\n",
    "resued for future runs, e.g. for different tomographic bins.\n",
    "\n",
    "Since that is not the case here, we use the `YawCacheDrop.drop()` method to\n",
    "delete the cached data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YawCacheDrop.make_stage().drop(cache_unk)\n",
    "YawCacheDrop.make_stage().drop(cache_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaw_rail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
