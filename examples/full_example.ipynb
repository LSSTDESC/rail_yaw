{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering redshifts with *yet_another_wizz*\n",
    "\n",
    "This notebooks summarises the steps to compute clustering redshifts for an\n",
    "unknown sample of galaxies using a reference sample with known redshifts.\n",
    "Additionally, a correction for the galaxy bias of the reference sample is\n",
    "applied (see Eqs. 17 & 20 in\n",
    "[van den Busch et al. 2020](https://arxiv.org/pdf/2007.01846)).\n",
    "\n",
    "This involves a number of steps:\n",
    "1. Preparing the input data (creating randoms, applying masks; simplfied here).\n",
    "2. Splitting the data into spatial patches and cache them on disk for faster access.\n",
    "3. Computing the autocorrelation function amplitude $w_{\\rm ss}(z)$, used as\n",
    "   correction for the galaxy bias\n",
    "4. Computing the cross-correlation function amplitude $w_{\\rm sp}(z)$, which is\n",
    "   the biased redshift estimate.\n",
    "5. Summarising the result by correcting for the refernece sample bias and\n",
    "   produce a redshift PDF.\n",
    "6. Removing the cached data which is no longer needed.\n",
    "\n",
    "At the very end, all stages are linked together to produce a `pipeline.yaml`\n",
    "file for reference.\n",
    "\n",
    "The aim of this notebook is to **give an overview of the wrapper functionality**,\n",
    "including a summary of all currently implemented optional parameters (commented).\n",
    "It is not meant to be a demonstaration of the performance of *yet_another_wizz*\n",
    "since the example data used here is very small and the resulting signal-to-noise\n",
    "ratio is quite poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.core.data import TableHandle\n",
    "\n",
    "# configure RAIL datastore to allow overwriting data\n",
    "from rail.core.stage import RailStage\n",
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "from yaw import UniformRandoms\n",
    "from rail.yaw_rail.utils import get_dc2_test_data\n",
    "\n",
    "from rail.estimation.algos.cc_yaw import (\n",
    "    stage_helper,  # utility for YawCacheCreate\n",
    "    YawCacheCreate,     # step 2\n",
    "    YawAutoCorrelate,   # step 3\n",
    "    YawCrossCorrelate,  # step 4\n",
    "    YawSummarize,       # step 5\n",
    "    YawCacheDrop,       # step 6\n",
    ")  # equivalent: from rail.yaw_rail import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = \"debug\"  # verbosity level of built-in logger, disable with \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "Since this is a simple example, we are not going into the details of creating\n",
    "realistic randoms and properly masking the reference and unknown data to a\n",
    "shared footprint on sky. Instead, we are using a simulated dataset that serves\n",
    "as both, reference and unknown sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the small test dataset derived from 25 sqdeg of DC2,\n",
    "containing 100k objects on a limited redshift range of $0.2 < z < 1.8$. We add\n",
    "the data as new handle to the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_dc2_test_data()  # downloads test data, cached for future calls\n",
    "redshifts = test_data[\"z\"].to_numpy()\n",
    "zmin = redshifts.min()\n",
    "zmax = redshifts.max()\n",
    "n_data = len(test_data)\n",
    "print(f\"N={n_data}, {zmin:.1f}<z<{zmax:.1f}\")\n",
    "\n",
    "test_data_handle = DS.add_data(\"input_data\", test_data, TableHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we generate a x10 enhanced uniform random dataset for the test data\n",
    "constrained to its rectangular footprint. We add redshifts by cloning the\n",
    "redshift column `\"z\"` of the dataset.  We add the randoms as new handle to the\n",
    "datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angular_rng = UniformRandoms(\n",
    "    test_data[\"ra\"].min(),\n",
    "    test_data[\"ra\"].max(),\n",
    "    test_data[\"dec\"].min(),\n",
    "    test_data[\"dec\"].max(),\n",
    "    seed=12345,\n",
    ")\n",
    "test_rand = angular_rng.generate(n_data * 10, draw_from=dict(z=redshifts))\n",
    "\n",
    "test_rand_handle = DS.add_data(\"input_rand\", test_rand, TableHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting and caching the data\n",
    "\n",
    "This step is crucial to compute consistent clustering redshift uncertainties.\n",
    "*yet_another_wizz* uses spatial (jackknife) resampling and therefore, every\n",
    "input dataset must be split into the same exact spatial regions/patches. To\n",
    "improve the parallel performance, the datasets and randoms are pre-arranged into\n",
    "these patches and cached on disk for better random patch-wise access. While this\n",
    "is slow for small datasets, it is highly beneficial for large datasets with many\n",
    "patches and/or memory constraints.\n",
    "\n",
    "The RAIL wrapper uses manually specified cache directories, each of which contains\n",
    "one dataset and optionally corresponding randoms. This ensures that the patch\n",
    "centers are defined consistently. To create a new cache, use the\n",
    "`YawCacheCreate.create()` method.\n",
    "\n",
    "### Note on names and aliasing in RAIL\n",
    "\n",
    "We need to create separate caches for the reference and the unknown data, which\n",
    "means that we need to run the `YawCacheCreate` twice. Since that creates name\n",
    "clashes in the RAIL datastore, we need to properly alias the inputs (`data`/\n",
    "`rand`) and the output (`cache`) by providing a dictionary for the `aliases`\n",
    "parameter when calling the `make_stage()`, e.g. by adding a unique suffix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"stage_name\"\n",
    "aliases = dict(data=\"data_suffix\", rand=\"rand_suffix\", cache=\"cache_suffix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a shorthand for convenience (`from rail.yaw_rail.cache.AliasHelper`)\n",
    "that allows to generate this dictionary by just providing a suffix name for the\n",
    "stage instance (see example below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"stage_name\"\n",
    "aliases = stage_helper(\"suffix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reference data\n",
    "\n",
    "To create a cache directory we must specify a `path` to the directory at which the\n",
    "data will be cached. This directory must not exist yet. We also have to specify\n",
    "a `name` for the stage to ensure that the reference and unknown caches (see below)\n",
    "are properly aliased to be distinguishable by the RAIL datastore.\n",
    "\n",
    "Furthermore, a few basic column names that describe the tabular input data must\n",
    "be provided. These are right ascension (`ra_name`) and declination (`dec_name`),\n",
    "and in case of the reference sample also the redshifts (`redshift_name`).\n",
    "Finally, the patches must be defined and there are three ways to do so:\n",
    "1. `n_patches`: Generating a given number of patches from the object positions\n",
    "   (peferrably of the randoms if possible) using k-means clustering.\n",
    "2. `patch_name`: Providing a column name in the input table which contains patch\n",
    "   indices (using 0-based indexing).\n",
    "3. `patches`: Using the patch centers from a different cache instance, given by\n",
    "   its path in the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_cache_ref = YawCacheCreate.make_stage(\n",
    "    name=\"cache_ref\",\n",
    "    aliases=stage_helper(\"ref\"),\n",
    "    path=\"run/test_ref\",\n",
    "    overwrite=True,  # default: False\n",
    "    ra_name=\"ra\",\n",
    "    dec_name=\"dec\",\n",
    "    redshift_name=\"z\",\n",
    "    # weight_name=,\n",
    "    # patches=,\n",
    "    # patch_name=,\n",
    "    n_patches=5,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")\n",
    "cache_ref = stage_cache_ref.create(\n",
    "    data=test_data_handle,\n",
    "    rand=test_rand_handle,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the log messages that *yet_another_wizz* processes the randoms\n",
    "first and generates patch centers (`creating 5 patches`) and then applies them\n",
    "to the dataset, which is processed last (`applying 5 patches`). Caching the data\n",
    "can take considerable time depending on the hardware and the number of patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The unknown data\n",
    "\n",
    "The same procedure for the unknown sample, however there are some small, but\n",
    "important differences. We use a different `path` and `name`, do not specify the\n",
    "`redshift_name` (since we would not have this information with real data), and\n",
    "here we chose to not provide any randoms for the unknown sample and instead rely\n",
    "on the reference sample randoms for cross-correlation measurements.\n",
    "\n",
    "Most importantly, we must provide the path to the reference data cache\n",
    "(`patches=\"./test_ref\"` instead of `n_patches`) since we must ensure that the\n",
    "patch centers are identical. Even if the reference and unknown data are the same\n",
    "as in this case, the automatically generated patch centers are not deterministic.\n",
    "We can see in the log messages that the code looks up the catalog in\n",
    "`./test_ref/rand` and reports `applying 5 patches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_cache_unk = YawCacheCreate.make_stage(\n",
    "    name=\"cache_unk\",\n",
    "    aliases=stage_helper(\"unk\"),\n",
    "    path=\"run/test_unk\",\n",
    "    overwrite=True,  # default: False\n",
    "    ra_name=\"ra\",\n",
    "    dec_name=\"dec\",\n",
    "    # redshift_name=,\n",
    "    # weight_name=,\n",
    "    patches=\"run/test_ref\",\n",
    "    # patch_name=,\n",
    "    # n_patches=,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")\n",
    "cache_unk = stage_cache_unk.create(\n",
    "    data=test_data_handle,\n",
    "    # rand=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing the autocorrelation / bias correction\n",
    "\n",
    "The bias correction is computed from the amplitude of the angular autocorrelation\n",
    "function of the reference sample. The measurement parameters are the same as for\n",
    "the cross-correlation amplitude measurement, so we can define all configuration\n",
    "parameters once in a dictionary.\n",
    "\n",
    "As a first step, we need to decide on which redshift bins we want to compute the\n",
    "clustering redshifts. Here we choose the redshift limits of the reference data\n",
    "(`zmin`/`zmax`) and, since the sample is small, only 8 bins (`zbin_num`) spaced\n",
    "linearly in redshift (default `method=\"linear\"`). Finally, we have to define the\n",
    "physical scales in kpc (`rmin`/`rmax`, converted to angular separation at each\n",
    "redshift) on which we measure the correlation amplitudes.\n",
    "\n",
    "\n",
    "**Optional parameters:** Bins edges can alternatively specifed manually through\n",
    "`zbins`. To apply scale dependent weights, e.g. $w \\propto r^{-1}$, specify\n",
    "the power-law exponent as`rweight=-1`. The parameter `rbin_num` specifies the\n",
    "radial resolution (logarithmic) of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_config = dict(\n",
    "    rmin=100,   # in kpc\n",
    "    rmax=1000,  # in kpc\n",
    "    # rweight=None,\n",
    "    # rbin_num=50,\n",
    "    zmin=zmin,\n",
    "    zmax=zmax,\n",
    "    zbin_num=8,  # default: 30\n",
    "    # method=\"linear\",\n",
    "    # zbins=np.linspace(zmin, zmax, zbin_num+1)\n",
    "    # crosspatch=True,\n",
    "    # thread_num=None,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then measure the autocorrelation using the `YawAutoCorrelate.correlate()`\n",
    "method, which takes a single parameter, the cache (handle) of the reference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_autocorr = YawAutoCorrelate.make_stage(\n",
    "    name=\"autocorr\",\n",
    "    **corr_config,\n",
    ")\n",
    "w_ss = stage_autocorr.correlate(\n",
    "    sample=cache_ref,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the code is progressing, we can observe the log messages of *yet_another_wizz*\n",
    "which indicate the performed steps: getting the cached data, generating a pairs\n",
    "of spatial patches to correlate, and counting pairs. Finally, the pair counts\n",
    "are stored as custom data handle in the datastore.\n",
    "\n",
    "We can interact with the returned pair counts these manually if we want to\n",
    "investigate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_ss = w_ss.data  # extract payload from handle\n",
    "counts_ss.dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computing the cross-correlation / redshift estimate\n",
    "\n",
    "The cross-correlation amplitude, which is the biased estimate of the unknown\n",
    "redshift distribution, is computed similarly to the autocorrelation above. We\n",
    "measure the correlation using the `YawCrossCorrelate.correlate()` method, which\n",
    "takes two parameters, the cache (handles) of the reference and the unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_crosscorr = YawCrossCorrelate.make_stage(\n",
    "    name=\"crosscorr\",\n",
    "    **corr_config,\n",
    ")\n",
    "w_sp = stage_crosscorr.correlate(\n",
    "    reference=cache_ref,\n",
    "    unknown=cache_unk,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can see the actions performed by *yet_another_wizz*. The main\n",
    "difference for the cross-correlation function is that the second sample (the\n",
    "unknown data/randoms) are not binned by redshift when counting pairs.\n",
    "\n",
    "As before, we can interact with the result, e.g. by evaluating the correlation\n",
    "estimator manually and getting the cross-correlation amplitude per redshift bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_sp = w_sp.data  # extract payload from handle\n",
    "corr_sp = counts_sp.sample()  # evaluate the correlation estimator\n",
    "corr_sp.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computing the redshift PDF\n",
    "\n",
    "The final analysis step is combining the two measured correlation amplitudes to\n",
    "get a redshift estimate which is corrected for the reference sample bias and\n",
    "converting it to a proper PDF. The latter is not trivial and different methods\n",
    "have been tested in the literature to remove negative correlation amplitudes.\n",
    "For simplicity, there is currently just one method implemented in the\n",
    "*yet_another_wizz* wrapper, which sets all negative or non-finite values to\n",
    "zero.\n",
    "\n",
    "We use `YawSummarize.summarize()` method, which takes the pair count handles of\n",
    "the cross- and autocorrelation functions as input. In principle, the\n",
    "autocorrelation of the unknown sample could be specified to fully correct for\n",
    "galaxy bias, however this is not possible in practice since the exact redshifts\n",
    "of the unknown objects are not known.\n",
    "\n",
    "All stage parameters are optional and usually not required. They can be used to\n",
    "customise the correlation estimators used for each correlation function. They\n",
    "default to Landy-Szalay (`\"LS\"`) where possible, otherwise Davis-Peebles (`\"DP\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_estimate = YawSummarize.make_stage(\n",
    "    name=\"estimate\",\n",
    "    # cross_est=,\n",
    "    # ref_est=,\n",
    "    # unk_est=,\n",
    "    # crosspatch=True,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")\n",
    "estimate = stage_estimate.summarize(\n",
    "    cross_corr=w_sp,\n",
    "    ref_corr=w_ss,  # default: None\n",
    "    # unk_corr=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stage produces two outputs, a `qp.Ensemble`, tagged as `output` which\n",
    "contains jackknife samples of the clustering redshift estimate will all negative\n",
    "amplitudes set to zero, and the uncorrected redshift estimate, tagged as\n",
    "`yaw_cc`. Below is a comparison plot of the true redshift histogram, the\n",
    "raw estimate from *yet_another_wizz* and the first jackknife sample stored in\n",
    "the `qp.Ensemble`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble, yaw_cc = estimate[\"output\"].data, estimate[\"yaw_cc\"].data\n",
    "\n",
    "ax = ensemble.plot(xlim=[zmin, zmax], label=\"qp, sample 1\")\n",
    "test_data.hist(\"z\", bins=w_sp.data.edges, density=True, color=\"0.8\", ax=ax, label=\"true\")\n",
    "yaw_cc.normalised().plot(label=\"yet_another_wizz\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Remove caches\n",
    "\n",
    "The cached datasets are not automatically removed, since the algorithm does not\n",
    "know when they are no longer needed. Additionally, the reference data could be\n",
    "resued for future runs, e.g. for different tomographic bins.\n",
    "\n",
    "Since that is not the case here, we use the `YawCacheDrop.drop()` method to\n",
    "delete the cached data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_drop_ref = YawCacheDrop.make_stage(name=\"drop_ref\")\n",
    "stage_drop_ref.drop(cache_ref)\n",
    "\n",
    "stage_drop_unk  = YawCacheDrop.make_stage(name=\"drop_unk\")\n",
    "stage_drop_unk.drop(cache_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pipeline file\n",
    "\n",
    "To build a working `ceci` pipeline, it is important to give unique names to the\n",
    "stages defined in the sections above. By running the stages we implicitly\n",
    "connect the in- and outputs of the stages already. Therefore we only need to\n",
    "create the pipeline from the existing stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ceci\n",
    "\n",
    "\n",
    "pipe = ceci.Pipeline.interactive()\n",
    "stages = [\n",
    "    stage_cache_ref,\n",
    "    stage_cache_unk,\n",
    "    stage_autocorr, \n",
    "    stage_crosscorr,\n",
    "    stage_estimate,\n",
    "    stage_drop_ref,\n",
    "    stage_drop_unk,\n",
    "]\n",
    "   \n",
    "for stage_ in stages:\n",
    "    pipe.add_stage(stage_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to initialise the pipeline. Assuming that the inptus will be\n",
    "present in the datas tore before running this pipeline, we can initialise the\n",
    "pipeline with dummy values. The same applies to the unused, optional stage\n",
    "inputs `rand_unk` and `unk_corr`.\n",
    "\n",
    "Finally, we save the pipeline and read it back for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.initialize(\n",
    "    dict(\n",
    "        input_data=\"dummy.in\",\n",
    "        input_rand=\"dummy.in\",\n",
    "        rand_unk=\"/dev/null\",\n",
    "        unk_corr=\"/dev/null\",\n",
    "    ), \n",
    "    dict(\n",
    "        output_dir=\".\",\n",
    "        log_dir=\".\",\n",
    "        resume=False\n",
    "    ), \n",
    "    None,\n",
    ")\n",
    "pipe.save(\"pipeline.yml\")\n",
    "ceci.Pipeline.read(\"pipeline.yml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaw_rail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
